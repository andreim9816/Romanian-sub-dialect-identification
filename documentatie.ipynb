{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vFINAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPlw_UqFs0JQ",
        "colab_type": "text"
      },
      "source": [
        "# **Romanian sub-dialect identification**\n",
        "<br>\n",
        "\n",
        "#### Discriminate between the Moldavian and the Romanian dialects across different text genres (news versus tweets)\n",
        "\n",
        "#### Author: Manolache Andrei - 244\n",
        "<br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "  One of the most important sub-tasks in pattern classification are feature extraction and selection. Prior to fitting the model and using machine learning algorithms for training, we need to think about how to best represent a text document as a feature vector. A commonly used model in Natural Language Processing is the so-called **TF-IDF** (term frequency-inverse document frequency) model. \n",
        "  <br>\n",
        "  This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
        "<br>\n",
        "  TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
        "  1. **TF: Term Frequency**, which measures how frequently a term occurs in a document. <br>\n",
        "  $TF(t)$ = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
        "  1.**IDF: Inverse Document Frequency** of the word across a set of documents. This means, how common or rare a word is in the entire document set.   <br>\n",
        "  $IDF(t)$ = log(Total number of documents / Number of documents with term t in it).\n",
        "  <br><br>\n",
        "  $\\text{Tf-idf} = TF(t) \\cdot IDF(t)$\n",
        "\n",
        "This way, TF-IDF gives us a way to associate each word in a document with a number that represents how relevant each word is in that document. \n",
        "\n",
        "<br>\n",
        "\n",
        "Furthermore, for training the data-sample, we used two classifiers: **SVM** (Support Vector Machines) and **ComplementNB** (Complement Naive Bayes)\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Complement Naive Bayes**\n",
        "It is based on the Multinomial Naive Bayes classifier, but it improves upon the weakness of the Naive Bayes classifier by estimating parameters from data in all classes except the one which we are evaluating for. Here, there is one important parameter to be used:\n",
        "\n",
        "*   Alpha (used for smoothing the prediction)\n",
        "\n",
        "\n",
        "###**Support Vector Machines**\n",
        "A Support Vector Machine (SVM) is a classifier defined by a separating hyperplane. In other words, given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples. To separate two classes, there are many possible hyperplanes that could be chosen. The optimal hyperplane is the one that has a maximum margin(maximum distance between data points of both classes). For adjusting the prediction, there are a few parameters that cand be used: \n",
        "\n",
        "  1. Regularization parameter (penalizes missclasifed points)\n",
        "  1. Gamma parameter (defines how far the influence of other points is considered)\n",
        "  1. Kernel parameter (used for transforming data so that it can be linear-separated)\n",
        "\n",
        "<br>\n",
        "\n",
        "First, we import the libraries needed for the classifiers and scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnOkR1t8wSq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "my_token_pattern = r\"(?u)\\S\\S+\" # defining a token pattern, used for TFIDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGg_csRwXkAk",
        "colab_type": "text"
      },
      "source": [
        "Function that opens a file (path) and returns a list of strings, each string representing a single line\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYy7-8TBXjpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readFile(path):\n",
        "\n",
        "    file = open(path , 'r' , encoding = \"utf-8\")\n",
        "    return file.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlyjkehGYRYT",
        "colab_type": "text"
      },
      "source": [
        "The files are opened, read and being returned a list of strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2v70M0DYRjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = readFile('train_samples.txt')\n",
        "test_samples = readFile('test_samples.txt')\n",
        "validation_samples = readFile('validation_samples.txt')\n",
        "train_labels = readFile('train_labels.txt')\n",
        "validation_labels = readFile('validation_labels.txt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7i8C0dmYxoW",
        "colab_type": "text"
      },
      "source": [
        "Gets the ids from the test samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1G1CgqvYxyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ids = [x for x in test_samples]\n",
        "test_ids = [int(x.split('\\t')[0]) for x in test_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJZLZuVdZhDF",
        "colab_type": "text"
      },
      "source": [
        "Function that gets the labels from samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sidEhUpRZqwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels(file):\n",
        "\n",
        "    labels = [int(label.split('\\t')[1]) for label in file]\n",
        "    return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r3rZDqjZHtg",
        "colab_type": "text"
      },
      "source": [
        "Gets the labels for the train and validations samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JYJhMcCZH1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = get_labels(train_labels)\n",
        "validation_labels = get_labels(validation_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxCkFHBZ3WT",
        "colab_type": "text"
      },
      "source": [
        "Function that gets rid of the samples tweets ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdAnb0Q4Z3ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_list_strings(dataSample):\n",
        "\n",
        "    finalString = [row.split('\\t')[1].replace('\\n' , '') for row in dataSample]\n",
        "    return finalString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO4CAV5cPX7l",
        "colab_type": "text"
      },
      "source": [
        "Updates the arrays, getting only the samples, without its:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL4RkWuPZ34R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = convert_to_list_strings(train_samples)\n",
        "test_samples = convert_to_list_strings(test_samples)\n",
        "validation_samples = convert_to_list_strings(validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ78-AH8p9RP",
        "colab_type": "text"
      },
      "source": [
        "Function that preprocesses the samples, spliting the words and keeping only those with length > 2 (So as to get rid of prepositions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bzN8mLFp50d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_preprocessor(text):\n",
        "\n",
        "    words = re.split(\"\\\\s+\", text)\n",
        "    words = [word.replace(\"\\n\", \"\") for word in words if len(word) > 2] # gets rid of the '\\n' character and keeps only the words of length > 2\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycfQbYdml0ud",
        "colab_type": "text"
      },
      "source": [
        "Defines the Tfidf processor, using parameters:\n",
        "1. 'l2' norm\n",
        "1. custom token pattern (for tokenizing the words)\n",
        "1. lowercase = False (in this case, a lower letter is not correlated with an upper one)\n",
        "1. custom preprocessor function (used for spliting the words)\n",
        "\n",
        "Then, it tokenizes each of the iterable string in words, creates a vocabulary of words and creates the tf-idf word count vector\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H787Xcqvl0WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(analyzer = 'word' , norm = 'l2' , token_pattern = my_token_pattern , lowercase = False , preprocessor = my_preprocessor )\n",
        "vector = vectorizer.fit_transform(train_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px-1fgbq2Utc",
        "colab_type": "text"
      },
      "source": [
        "Function that generates a term document matrix for the samples, using the generated vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E80aaVWJ2oDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(samples, vectorizer):\n",
        "    return vectorizer.transform(samples).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnPesHaY2xJv",
        "colab_type": "text"
      },
      "source": [
        "Generates term document matrixes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia282ZZX2xUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = transform(train_samples, vectorizer)\n",
        "test_samples = transform(test_samples, vectorizer)\n",
        "validation_samples = transform(validation_samples, vectorizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cqL_C1-t6wU",
        "colab_type": "text"
      },
      "source": [
        "### Now, we start training the first classifier: **SVM**\n",
        "<br>\n",
        "\n",
        "We'll use the **linearSVC** classifier, an improved implementation of the SVC classifier with **kernel = 'linear'** parameter, with different values for the regularization parameter C:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7kbc_AOujis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8bf3968c-562f-499a-ca1f-2243b03759df"
      },
      "source": [
        "data = [] # array for the accuracy and f1 score of the predictions\n",
        "\n",
        "for c in [0.001 , 0.01 , 0.1 , 1, 10, 100, 1000]:\n",
        "    svmClassifier = svm.LinearSVC(C = c)\n",
        "    svmClassifier.fit(train_samples , train_labels)\n",
        "    predictions = svmClassifier.predict(validation_samples)\n",
        "    data.append([c , accuracy_score(validation_labels , predictions) , f1_score(validation_labels , predictions)])\n",
        "\n",
        "dataFrame = pd.DataFrame(data , columns = ['C ' , ' Accuracy ' , ' F1 Score'])\n",
        "print(dataFrame)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         C    Accuracy    F1 Score\n",
            "0     0.001    0.564759   0.698172\n",
            "1     0.010    0.650226   0.707586\n",
            "2     0.100    0.680723   0.709788\n",
            "3     1.000    0.668298   0.681835\n",
            "4    10.000    0.650979   0.664009\n",
            "5   100.000    0.648720   0.661097\n",
            "6  1000.000    0.647590   0.659636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQOfh3dO-aj3",
        "colab_type": "text"
      },
      "source": [
        "We notice that the results are increasing and then decreasing, with its peak point of best accuracy / score for **C = 0.1**, with an accuracy of **0.680723**\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Next, we'll check the **complementNB** classifier, adjusting the alpha hyperparamater:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs0VljZE_KLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2607e4cd-1a8b-4d27-f66f-822d2336ded8"
      },
      "source": [
        "data = [] # array for the accuracy and f1 score of the predictions\n",
        "\n",
        "for alph in [0.01 , 0.1 , 0.26 , 0.3 , 0.5 , 1 , 10 , 25 , 50]:\n",
        "    complementNBClassifier = ComplementNB(alpha = alph)\n",
        "    complementNBClassifier.fit(train_samples , train_labels)\n",
        "    predictions = complementNBClassifier.predict(validation_samples)\n",
        "    data.append([alph , accuracy_score(validation_labels , predictions) , f1_score(validation_labels , predictions)])\n",
        "    \n",
        "dataFrame = pd.DataFrame(data , columns = ['C ' , '   Accuracy ' , ' F1 Score'])\n",
        "print(dataFrame)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      C      Accuracy    F1 Score\n",
            "0   0.01      0.718750   0.729446\n",
            "1   0.10      0.716867   0.732765\n",
            "2   0.26      0.716114   0.732434\n",
            "3   0.30      0.717244   0.732645\n",
            "4   0.50      0.710467   0.723878\n",
            "5   1.00      0.707078   0.719134\n",
            "6  10.00      0.671687   0.662016\n",
            "7  25.00      0.660392   0.637168\n",
            "8  50.00      0.649473   0.614812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5QIQCOUMBy_",
        "colab_type": "text"
      },
      "source": [
        "We notice that the accuracy is decreasing, but the F1 score is gaing a peak point for **alpha = 0.3**, with an accuracy of **0.717244** and F1 score of **0.732645**.\n",
        "<br>\n",
        "In conclusion, we notice that the best results were accomplished using the **complementNB** classifier, with the best results for the alpha parameter **0.3** and **0.26**. \n",
        "\n",
        "<br>\n",
        "\n",
        "So, the F1 score and confusion matrix for complementNB classifier and alpha = 0.26 are:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqZmxtqjSC8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "865fd7b9-c7a2-40a9-f35a-cccfda851c16"
      },
      "source": [
        "complementNBClassifier = ComplementNB(alpha = 0.26)\n",
        "complementNBClassifier.fit(train_samples , train_labels)\n",
        "predictions = complementNBClassifier.predict(validation_samples)\n",
        "print('F1 score is ' , f1_score(validation_labels , predictions))\n",
        "print('Confusion matrix for ComplementNB and alpha = 0.26: ')\n",
        "print(confusion_matrix(validation_labels , predictions) , '\\n\\n')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score is  0.7324343506032648\n",
            "Confusion matrix for ComplementNB and alpha = 0.26: \n",
            "[[ 870  431]\n",
            " [ 323 1032]] \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qD7HQGfVAI8",
        "colab_type": "text"
      },
      "source": [
        "F1 score and confusion matrix for complementNB classifier and alpha = 0.3 are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keie38hCVJo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7fe8d69b-0a9c-49a5-fdaf-34f748cd5712"
      },
      "source": [
        "complementNBClassifier = ComplementNB(alpha = 0.3)\n",
        "complementNBClassifier.fit(train_samples , train_labels)\n",
        "predictions = complementNBClassifier.predict(validation_samples)\n",
        "print('F1 score is ' , f1_score(validation_labels , predictions))\n",
        "print('Confusion matrix for ComplementNB and alpha = 0.3: ')\n",
        "print(confusion_matrix(validation_labels , predictions) , '\\n\\n')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score is  0.7326450694197223\n",
            "Confusion matrix for ComplementNB and alpha = 0.3: \n",
            "[[ 876  425]\n",
            " [ 326 1029]] \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN9JYUhAVpIl",
        "colab_type": "text"
      },
      "source": [
        "So, for predicting the test samples, we'll use the **complementNB** classifier, with **alpha = 0.3** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOpscEDlVpb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complementNBClassifier = ComplementNB(alpha = 0.3)\n",
        "complementNBClassifier.fit(train_samples , train_labels)\n",
        "predictions_test = complementNBClassifier.predict(test_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXC_rDWFXuq5",
        "colab_type": "text"
      },
      "source": [
        "And generate the output file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh51KnAJXxgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csvFile = pd.DataFrame({ \"id\" : test_ids , \"label\" : predictions_test})\n",
        "csvFile.to_csv(\"results.csv\" , index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXP8jw-WZfTL",
        "colab_type": "text"
      },
      "source": [
        "### Concluding Remarks\n",
        "\n",
        "For this dialect classification project, I used the TfidfVectorizer technique to correlate each word with a score, depending on its frequency / number of appearances in the documents and samples. I compared 2 types of classifiers: linearSVC(SVM) and complementNB(Naive Bayes) with different parameters, concluding that the one with the best results is complementNB with alpha = 0.3.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Bibliography and Research\n",
        "For this project, along the ideas explained at the course / laboratory, I used some concepts from the following websites:\n",
        "\n",
        "*   https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "*   https://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "*   https://blog.floydhub.com/naive-bayes-for-machine-learning/\n",
        "*   https://sebastianraschka.com/Articles/2014_naive_bayes_1.html\n",
        "*   https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}